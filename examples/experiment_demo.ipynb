{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up a Simple Experiment\n",
    "\n",
    "This is a complete example of how one would test a human subject on a small set of TSPs, and compare human tours to optimal (Concorde) tours. Adapted from the [documentation for the `experiment` module](https://jackv.co/tsp/tsp/experiment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tsp.core.tsp import TSP\n",
    "from tsp.experiment.batch import save_problem_batch\n",
    "from tsp.experiment.batch_server import batch_server_run\n",
    "from tsp.experiment.batch_solver import solve_batch, score_batch_2\n",
    "#from tsp.core.solvers import concorde_solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate a set of problems (in this case, we will create a set of 10 20-city problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = []\n",
    "for i in range(10):\n",
    "    problems.append(TSP.generate_random(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, save the problem set in `test/problems`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_problem_batch(problems, 'test/problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the UI to collect solutions from the human (the UI can be accessed at [localhost:8080](http://localhost:8080/)). Save them in `test/human`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving on http://localhost:8080 ...\n",
      "Stopping server...\n"
     ]
    }
   ],
   "source": [
    "batch_server_run('test/problems', 'test/human', randomized=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Concorde solutions to the problem set, saving them in `test/concorde`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concorde_solve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m concorde_solutions = solve_batch(\u001b[33m'\u001b[39m\u001b[33mtest/problems\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mconcorde_solve\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mtest/concorde\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'concorde_solve' is not defined"
     ]
    }
   ],
   "source": [
    "concorde_solutions = solve_batch('test/problems', concorde_solve, 'test/concorde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can generate the errors when comparing the human tours to the optimal tours produced by Concorde. `errors` will store an array of the errors for the 10 problems, `mean` the mean of the 10 errors, and `ste` the standard error of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_batch_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m errors, mean, ste \u001b[38;5;241m=\u001b[39m \u001b[43mscore_batch_2\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/problems\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/human\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/concorde\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_batch_2' is not defined"
     ]
    }
   ],
   "source": [
    "errors, mean, ste = score_batch_2('test/problems', 'test/human', 'test/concorde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple visualization in which the problems are rank-ordered by human error, and displayed with problem number on the abscissa and error on the ordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mHuman Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmean\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m +/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mste\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m plt.plot(np.arange(\u001b[32m1\u001b[39m, \u001b[32m11\u001b[39m), \u001b[38;5;28msorted\u001b[39m(errors), \u001b[33m'\u001b[39m\u001b[33mo-\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m plt.xticks(np.arange(\u001b[32m1\u001b[39m, \u001b[32m11\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Human Error: {mean:.3f} +/- {ste:.3f}')\n",
    "plt.plot(np.arange(1, 11), sorted(errors), 'o-')\n",
    "plt.xticks(np.arange(1, 11))\n",
    "plt.xlabel('Problem #')\n",
    "plt.ylabel('Human Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
